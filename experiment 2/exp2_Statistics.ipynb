{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr9sflOpDZIG",
        "outputId": "c0711582-22f0-4462-805a-47d5ac6d85bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------+-----+-----+----+------+------+----+---------+\n",
            "|Sno|           Name|Quiz1|Quiz2|AVG1|Assig1|Assig2|AVG2| Total 75|\n",
            "+---+---------------+-----+-----+----+------+------+----+---------+\n",
            "|  1|  Ashir Mehfooz| 14.0| 14.0|14.0|  13.0|  13.0|13.0|    68.00|\n",
            "|  2|    Atif Raftad|  4.0| 10.0| 7.0|   4.0|   5.0| 4.5|    41.50|\n",
            "|  3|     Saiqa Aziz| 15.0| 11.0|13.0|  14.0|  13.0|13.5|    60.50|\n",
            "|  8|   Ozair Minhas|  6.0|  5.0| 5.5|   4.0|   6.0| 5.0|    22.50|\n",
            "|  9|Naveera Subhani|  5.0| 11.0| 8.0|   4.0|   5.0| 4.5|    46.50|\n",
            "+---+---------------+-----+-----+----+------+------+----+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+------------------------+---------+---------+\n",
            "|       Mean Quiz1|Standard Deviation Quiz1|Max Quiz1|Min Quiz1|\n",
            "+-----------------+------------------------+---------+---------+\n",
            "|8.208955223880597|       4.575600441798329|     15.0|      0.0|\n",
            "+-----------------+------------------------+---------+---------+\n",
            "\n",
            "+-----------------+------------------------+---------+---------+\n",
            "|       Mean Quiz2|Standard Deviation Quiz2|Max Quiz2|Min Quiz2|\n",
            "+-----------------+------------------------+---------+---------+\n",
            "|8.109452736318408|       4.657033411841166|     15.0|      0.0|\n",
            "+-----------------+------------------------+---------+---------+\n",
            "\n",
            "+-----------------+-----------------------+--------+--------+\n",
            "|        Mean Avg1|Standard Deviation Avg1|Max Avg1|Min Avg1|\n",
            "+-----------------+-----------------------+--------+--------+\n",
            "|8.159203980099502|     3.6471944509696876|    15.0|     0.0|\n",
            "+-----------------+-----------------------+--------+--------+\n",
            "\n",
            "+------------------+-------------------------+----------+----------+\n",
            "|       Mean Assig1|Standard Deviation Assig1|Max Assig1|Min Assig1|\n",
            "+------------------+-------------------------+----------+----------+\n",
            "|7.8606965174129355|        4.687269729004061|      15.0|       0.0|\n",
            "+------------------+-------------------------+----------+----------+\n",
            "\n",
            "+-----------------+-------------------------+----------+----------+\n",
            "|      Mean Assig2|Standard Deviation Assig2|Max Assig2|Min Assig2|\n",
            "+-----------------+-------------------------+----------+----------+\n",
            "|8.059701492537313|         5.31473592104516|      44.0|       0.0|\n",
            "+-----------------+-------------------------+----------+----------+\n",
            "\n",
            "+-----------------+-----------------------+--------+--------+\n",
            "|        Mean Avg2|Standard Deviation Avg2|Max Avg2|Min Avg2|\n",
            "+-----------------+-----------------------+--------+--------+\n",
            "|7.960199004975125|     3.8004483893613137|    23.0|     0.0|\n",
            "+-----------------+-----------------------+--------+--------+\n",
            "\n",
            "+-----------------------+\n",
            "|Correlation Quiz1 Quiz2|\n",
            "+-----------------------+\n",
            "|    0.24834968491901308|\n",
            "+-----------------------+\n",
            "\n",
            "Covariance Quiz1 Quiz2: 5.292014925373135\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import mean, stddev, col, max, min, isnan,corr\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Statistical Measures and Probability Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the dataset and remove rows with null values\n",
        "data_path = \"marks.csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True).dropna()\n",
        "\n",
        "# Show the first 5 rows of the DataFrame\n",
        "df.show(5)\n",
        "\n",
        "# Calculate mean, standard deviation, max, and min for specific columns\n",
        "df.select(mean(col(\"Quiz1\")).alias(\"Mean Quiz1\"),\n",
        "          stddev(col(\"Quiz1\")).alias(\"Standard Deviation Quiz1\"),\n",
        "          max(col(\"Quiz1\")).alias(\"Max Quiz1\"),\n",
        "          min(col(\"Quiz1\")).alias(\"Min Quiz1\")).show()\n",
        "\n",
        "df.select(mean(col(\"Quiz2\")).alias(\"Mean Quiz2\"),\n",
        "          stddev(col(\"Quiz2\")).alias(\"Standard Deviation Quiz2\"),\n",
        "          max(col(\"Quiz2\")).alias(\"Max Quiz2\"),\n",
        "          min(col(\"Quiz2\")).alias(\"Min Quiz2\")).show()\n",
        "\n",
        "df.select(mean(col(\"Avg1\")).alias(\"Mean Avg1\"),\n",
        "          stddev(col(\"Avg1\")).alias(\"Standard Deviation Avg1\"),\n",
        "          max(col(\"Avg1\")).alias(\"Max Avg1\"),\n",
        "          min(col(\"Avg1\")).alias(\"Min Avg1\")).show()\n",
        "\n",
        "df.select(mean(col(\"Assig1\")).alias(\"Mean Assig1\"),\n",
        "          stddev(col(\"Assig1\")).alias(\"Standard Deviation Assig1\"),\n",
        "          max(col(\"Assig1\")).alias(\"Max Assig1\"),\n",
        "          min(col(\"Assig1\")).alias(\"Min Assig1\")).show()\n",
        "\n",
        "df.select(mean(col(\"Assig2\")).alias(\"Mean Assig2\"),\n",
        "          stddev(col(\"Assig2\")).alias(\"Standard Deviation Assig2\"),\n",
        "          max(col(\"Assig2\")).alias(\"Max Assig2\"),\n",
        "          min(col(\"Assig2\")).alias(\"Min Assig2\")).show()\n",
        "\n",
        "df.select(mean(col(\"Avg2\")).alias(\"Mean Avg2\"),\n",
        "          stddev(col(\"Avg2\")).alias(\"Standard Deviation Avg2\"),\n",
        "          max(col(\"Avg2\")).alias(\"Max Avg2\"),\n",
        "          min(col(\"Avg2\")).alias(\"Min Avg2\")).show()\n",
        "\n",
        "# Calculate correlation between two columns\n",
        "df.select(corr(\"Quiz1\", \"Quiz2\").alias(\"Correlation Quiz1 Quiz2\")).show()\n",
        "\n",
        "# Calculate covariance between two columns\n",
        "covariance = df.stat.cov(\"Quiz1\", \"Quiz2\")\n",
        "print(f\"Covariance Quiz1 Quiz2: {covariance}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt6p-dgqGAyb",
        "outputId": "2b830db9-3adb-4935-b18d-8d5155de1e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=f0a6568daf10ee7fc45bae9c58cfe3109b1ee4466716aaef309e9cfe09918728\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, calculate the necessary probabilities: P(Quiz1), P(Quiz2), and P(Quiz2|Quiz1)\n",
        "p_quiz1 = df.filter(df['Quiz1'] >= 10).count() / df.count()\n",
        "p_quiz2_given_quiz1 = df.filter((df['Quiz1'] >= 10) & (df['Quiz2'] >= 10)).count() / df.filter(df['Quiz1'] >= 10).count()\n",
        "p_quiz2 = df.filter(df['Quiz2'] >= 10).count() / df.count()\n",
        "\n",
        "# Apply Bayes' Theorem: P(Quiz1|Quiz2) = (P(Quiz2|Quiz1) * P(Quiz1)) / P(Quiz2)\n",
        "p_quiz1_given_quiz2 = (p_quiz2_given_quiz1 * p_quiz1) / p_quiz2\n",
        "print(f\"P(Quiz1|Quiz2): {p_quiz1_given_quiz2}\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "-1tGPuqILvdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72902c66-44ab-4d2f-817f-aa91f53f9b46"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(Quiz1|Quiz2): 0.6022727272727273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iEKCuYla3N4"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}