{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr9sflOpDZIG",
        "outputId": "98176fae-6e52-4792-c1b7-207489d194c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on test data = 77.64171277633882\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, hour\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Social Media Engagement Prediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load data into a DataFrame\n",
        "data_path = \"sentimentdataset.csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Encode 'Sentiment' column to numeric values\n",
        "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Convert 'Timestamp' column to datetime and extract relevant features\n",
        "df = df.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
        "df = df.withColumn(\"Year\", year(\"Timestamp\"))\n",
        "df = df.withColumn(\"Month\", month(\"Timestamp\"))\n",
        "df = df.withColumn(\"Day\", dayofmonth(\"Timestamp\"))\n",
        "df = df.withColumn(\"Hour\", hour(\"Timestamp\"))\n",
        "\n",
        "# Drop unnecessary columns\n",
        "drop_columns = ['Unnamed: 0', 'Text', 'Timestamp', 'User', 'Platform', 'Hashtags', 'Country']\n",
        "df = df.drop(*drop_columns)\n",
        "\n",
        "# Define features and label\n",
        "features = ['Retweets', 'Likes']\n",
        "label = 'label'\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
        "scalerModel = scaler.fit(df)\n",
        "df = scalerModel.transform(df)\n",
        "\n",
        "# Split data into train and test sets\n",
        "(train_data, test_data) = df.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Initialize and train the linear regression model\n",
        "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=label)\n",
        "lrModel = lr.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = lrModel.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt6p-dgqGAyb",
        "outputId": "2b830db9-3adb-4935-b18d-8d5155de1e1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=f0a6568daf10ee7fc45bae9c58cfe3109b1ee4466716aaef309e9cfe09918728\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8x6DRwuPGRhm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}